{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import copy\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "item_df = pd.read_csv('data/item_metadata.csv')\n",
    "submission_df = pd.read_csv('data/submission_popular.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please refer to https://recsys.trivago.cloud/challenge/dataset/ for better understanding <i>item metadata</i> and <i>session actions</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make 'item encoding' dictionary using <i>item metadata</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "for i in range(len(item_df)):\n",
    "    properties += item_df['properties'][i].split(\"|\")\n",
    "property_count = Counter(properties)\n",
    "property_set = list(property_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_df = pd.DataFrame(np.zeros([len(item_df), len(property_set)]), index=item_df['item_id'], columns=property_set)\n",
    "for i, row in tqdm(item_df.iterrows()):\n",
    "    item_id = row['item_id']\n",
    "    properties = row['properties'].split(\"|\")\n",
    "    onehot_df.loc[item_id][properties] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 50\n",
    "learning_rate = 5e-3\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "dataset = onehot_df.values\n",
    "loader = torch.utils.data.DataLoader(dataset=torch.tensor(dataset), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAE, self).__init__()\n",
    "        self.enc = nn.Linear(157, 32)\n",
    "        self.enc_act = nn.Tanh()\n",
    "        self.dec = nn.Linear(32, 157)\n",
    "        self.dec_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.enc_act(self.enc(x))\n",
    "        decoded = self.dec_act(self.dec(encoded))\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,train_loader,learning_rate,num_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 40], gamma=0.2)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            item_meta = data.type(torch.FloatTensor).cuda()\n",
    "            recon_item_meta = model(item_meta)[1]\n",
    "            loss = criterion(recon_item_meta, item_meta)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae = SimpleAE().cuda()\n",
    "fit(simple_ae, loader, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset=torch.tensor(dataset), batch_size=batch_size, shuffle=False)\n",
    "simple_ae.eval()\n",
    "encoding_lst = []\n",
    "for i, data in enumerate(loader):\n",
    "    item_meta = data.type(torch.FloatTensor).cuda()\n",
    "    encoding, decoding = simple_ae(item_meta)\n",
    "    \n",
    "    encoding = encoding.cpu().detach().numpy().tolist()\n",
    "    encoding_lst += encoding\n",
    "encoding_lst = np.array(encoding_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_encoding_dict = {}\n",
    "for i, item_id in enumerate(onehot_df.index):\n",
    "    item_encoding_dict[item_id] = encoding_lst[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/item_encoding_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(item_encoding_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the general properties of clicked items and make those properties dictionaries using <i>item metadata</i>\n",
    "### These properties already were used for item encoding, but they are important features so that we use the information before encoding again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions_idx = train_df[~train_df['impressions'].isna()].index\n",
    "clicked_items = list(set([int(item_id) for item_id in list(train_df.loc[impressions_idx]['reference'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_item_properties = []\n",
    "for item in tqdm(clicked_items):\n",
    "    try:\n",
    "        clicked_item_properties += list(item_df[item_df['item_id']==item]['properties'])[0].split(\"|\")\n",
    "    except:\n",
    "        pass\n",
    "clicked_item_property_count = Counter(clicked_item_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in clicked_item_property_count.items():\n",
    "    if v > 150000:\n",
    "        print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_hotel = {}\n",
    "item_star = {}\n",
    "item_tv = {}\n",
    "item_shower = {}\n",
    "item_window = {}\n",
    "item_car = {}\n",
    "item_wifi = {}\n",
    "item_nosmoke = {}\n",
    "for i in tqdm(range(len(item_df))):\n",
    "    item_id = item_df['item_id'][i]\n",
    "    properties = item_df['properties'][i].split(\"|\")\n",
    "    stars = [p for p in properties if 'Star' in p and len(p)==6]\n",
    "    if 'Hotel' in properties:\n",
    "        item_hotel[item_id] = 1\n",
    "        if len(stars) > 0:\n",
    "            item_star[item_id] = int(stars[0][0])\n",
    "        else:\n",
    "            item_star[item_id] = 0\n",
    "    else:\n",
    "        item_hotel[item_id] = 0\n",
    "        item_star[item_id] = 0\n",
    "    \n",
    "    item_tv[item_id] = 1 if 'Television' in properties else 0\n",
    "    item_shower[item_id] = 1 if 'Shower' in properties else 0\n",
    "    item_window[item_id] = 1 if 'Openable Windows' in properties else 0\n",
    "    item_car[item_id] = 1 if 'Car Park' in properties else 0\n",
    "    item_wifi[item_id] = 1 if 'WiFi (Public Areas)' in properties else 0\n",
    "    item_wifi[item_id] = 1 if 'WiFi (Rooms)' in properties else item_wifi[item_id]\n",
    "    item_nosmoke[item_id] = 1 if 'Non-Smoking Rooms' in properties else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make 'rating' dictionary using <i>item metadata</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "for i in range(len(item_df)):\n",
    "    properties += item_df['properties'][i].split(\"|\")\n",
    "properties = list(set(properties))\n",
    "ratings = [p for p in properties if 'Rating' in p]\n",
    "print (ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_rating = {}\n",
    "for i in range(len(item_df)):\n",
    "    item_id = item_df['item_id'][i]\n",
    "    properties = item_df['properties'][i].split(\"|\")\n",
    "    rating = [p for p in properties if p in ratings]\n",
    "    item_rating[item_id] = len(rating)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make 'viewed' feature\n",
    "### Binary check on items that the user has seen before (before session or before step within session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lst = list(set(list(set(train_df['user_id']))+list(set(test_df['user_id']))))\n",
    "user_items = {}\n",
    "for user in user_lst:\n",
    "    user_items[user] = []\n",
    "\n",
    "viewed_lst = []\n",
    "for i, row in tqdm(train_df.iterrows()):\n",
    "    u_id = row['user_id']\n",
    "    action = row['action_type']\n",
    "    ref = row['reference']\n",
    "    impressions = row['impressions']\n",
    "    \n",
    "    if action == \"clickout item\":\n",
    "        viewed = \"|\".join([str(1) if int(item) in user_items[u_id] else str(0) for item in impressions.split(\"|\")])\n",
    "        viewed_lst.append(viewed)\n",
    "    else:\n",
    "        viewed_lst.append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        if int(ref) not in user_items[u_id]:\n",
    "            user_items[u_id].append(int(ref))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "train_df['viewed'] = viewed_lst\n",
    "\n",
    "viewed_lst = []\n",
    "for i, row in tqdm(test_df.iterrows()):\n",
    "    u_id = row['user_id']\n",
    "    action = row['action_type']\n",
    "    ref = row['reference']\n",
    "    impressions = row['impressions']\n",
    "    \n",
    "    if action == \"clickout item\":\n",
    "        viewed = \"|\".join([str(1) if int(item) in user_items[u_id] else str(0) for item in impressions.split(\"|\")])\n",
    "        viewed_lst.append(viewed)\n",
    "    else:\n",
    "        viewed_lst.append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        if int(ref) not in user_items[u_id]:\n",
    "            user_items[u_id].append(int(ref))\n",
    "    except:\n",
    "        pass\n",
    "test_df['viewed'] = viewed_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete meaningless train dataset for us\n",
    "### delete sessions that have no 'clickout' action and delete the steps after the last 'clickout' action within a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_lst = []\n",
    "selected_idx = []\n",
    "\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    step = train_df['step'][i]\n",
    "    if i != 0 and step == 1:\n",
    "        first_idx = i-(len(act_lst))\n",
    "        clickout_idx = np.where(np.array(act_lst)=='clickout item')[0]\n",
    "        if len(clickout_idx) != 0:\n",
    "            last_idx = first_idx + clickout_idx[-1]\n",
    "            selected_idx += list(range(first_idx, last_idx+1))\n",
    "        else:\n",
    "            last_idx = -9999\n",
    "        act_lst = []\n",
    "    act = train_df['action_type'][i]\n",
    "    act_lst.append(act)\n",
    "    \n",
    "selected_idx += list(range(15932973, 15932992))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.DataFrame(train_df.loc[selected_idx], columns=train_df.columns)\n",
    "new_train_df = new_train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete meaningless test dataset for us\n",
    "### delete sessions that have no 'clickout' of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = []\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    step = test_df['step'][i]\n",
    "    action_type = test_df['action_type'][i]\n",
    "    ref = test_df['reference'][i]\n",
    "    if step == 1:\n",
    "        first_idx = i\n",
    "    if action_type == 'clickout item' and ref is np.nan:\n",
    "        last_idx = i\n",
    "        selected_idx += list(range(first_idx, last_idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = pd.DataFrame(test_df.loc[selected_idx], columns=test_df.columns)\n",
    "new_test_df = new_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add item features from dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df, item_dict):\n",
    "    impressions_idx = df[~df['impressions'].isna()].index\n",
    "    features_lst = []\n",
    "    prev_idx = -1\n",
    "    for idx in tqdm(impressions_idx):\n",
    "        impressions = df['impressions'][idx].split(\"|\")\n",
    "        tmp_features = []\n",
    "        for impression in impressions:\n",
    "            try:\n",
    "                tmp_features.append(str(item_dict[int(impression)]))\n",
    "            except:\n",
    "                tmp_features.append(str(0))\n",
    "        tmp_features = \"|\".join(tmp_features)\n",
    "        tmp_features_lst = [np.nan]*((idx-1)-prev_idx) + [tmp_features]\n",
    "        features_lst += tmp_features_lst\n",
    "        prev_idx = idx\n",
    "    return features_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ratings', 'hotel', 'star', 'tv', 'shower', 'window', 'car', 'wifi', 'nosmoke']\n",
    "features_dict = [item_rating, item_hotel, item_star, item_tv, item_shower, item_window, item_car, item_wifi, item_nosmoke]\n",
    "\n",
    "for name, feature_dict in list(zip(names, features_dict)):\n",
    "    new_train_df[name] = get_features(new_train_df, feature_dict)\n",
    "    new_test_df[name] = get_features(new_test_df, feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add 'resident time' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_resident_time(df):\n",
    "    timestamp_before_lst = np.array(df['timestamp'])[:-1]\n",
    "    timestamp_after_lst = np.array(df['timestamp'])[1:]\n",
    "    \n",
    "    resident_time_lst = timestamp_after_lst-timestamp_before_lst\n",
    "    resident_time_lst = np.append(resident_time_lst, [-9999])\n",
    "    \n",
    "    step1_idx = np.array(df[df['step']==1].index)\n",
    "    final_idx = step1_idx-1\n",
    "    final_idx = np.delete(final_idx, 0)\n",
    "    final_idx = np.append(final_idx, df.index[-1])\n",
    "    \n",
    "    resident_time_lst[final_idx] = -9999\n",
    "    return resident_time_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df['resident_time'] = add_resident_time(new_train_df)\n",
    "new_test_df['resident_time'] = add_resident_time(new_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add 'price difference' feature and 'rating difference' feature\n",
    "### 'price difference' is the difference between the accommodations on the screen and the price that the user has seen within the same session\n",
    "### 'rating difference' is the binary feature. The value is 1 if the rating of the accommodations on the screen is equal to or greater than the minimum rating that the user has seen within the same session, otherwise 0.\n",
    "### If user did not see any item, these features are filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_features(df):\n",
    "    impressions_idx = df[~df['impressions'].isna()].index\n",
    "    impressions_lst = []\n",
    "    prices_lst = []\n",
    "    ratings_lst = []\n",
    "    prev_idx = -1\n",
    "    for idx in tqdm(impressions_idx):\n",
    "        impressions = df['impressions'][idx]\n",
    "        tmp_impressions = [impressions] * (idx-prev_idx)\n",
    "        impressions_lst += tmp_impressions\n",
    "        \n",
    "        prices = df['prices'][idx]\n",
    "        tmp_prices = [prices] * (idx-prev_idx)\n",
    "        prices_lst += tmp_prices\n",
    "        \n",
    "        ratings = df['ratings'][idx]\n",
    "        tmp_ratings = [ratings] * (idx-prev_idx)\n",
    "        ratings_lst += tmp_ratings\n",
    "        \n",
    "        prev_idx = idx\n",
    "    return impressions_lst, prices_lst, ratings_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df['impressions'], new_train_df['prices'], new_train_df['ratings'] = copy_features(new_train_df)\n",
    "new_test_df['impressions'], new_test_df['prices'], new_test_df['ratings'] = copy_features(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diff(df):\n",
    "    diff_price_lst = []\n",
    "    diff_rating_lst = []\n",
    "    ref_price = {}\n",
    "    ref_rating = {}\n",
    "    for i in tqdm(range(len(df))):\n",
    "        ref = df['reference'][i]\n",
    "        impressions = df['impressions'][i].split(\"|\")\n",
    "        prices = df['prices'][i].split(\"|\")\n",
    "        ratings = df['ratings'][i].split(\"|\")\n",
    "        resident_time = df['resident_time'][i]\n",
    "        if resident_time != -9999:\n",
    "            diff_price_lst.append(np.nan)\n",
    "            diff_rating_lst.append(np.nan)\n",
    "            if ref in impressions:\n",
    "                ref_idx = impressions.index(ref)\n",
    "                ref_price[ref] = int(prices[ref_idx])\n",
    "                ref_rating[ref] = int(ratings[ref_idx])\n",
    "        else:\n",
    "            if len(ref_price) > 0:\n",
    "                avg_price = sum(ref_price.values())/len(ref_price)\n",
    "                min_rating = min(ref_rating.values())\n",
    "                diff_prices = '|'.join([str(int(p)-avg_price) for p in prices])\n",
    "                diff_ratings = '|'.join([str(1) if int(r)>=min_rating else str(0) for r in ratings])\n",
    "            else:\n",
    "                diff_prices = '|'.join([str(0)]*len(prices))\n",
    "                diff_ratings = '|'.join([str(0)]*len(ratings))\n",
    "            diff_price_lst.append(diff_prices)\n",
    "            diff_rating_lst.append(diff_ratings)\n",
    "            ref_price = {}\n",
    "            ref_rating = {}\n",
    "    return diff_price_lst, diff_rating_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df['diff_prices'], new_train_df['diff_ratings'] = calculate_diff(new_train_df)\n",
    "new_test_df['diff_prices'], new_test_df['diff_ratings'] = calculate_diff(new_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the steps that have or might have items on the screen of the steps from those of the last step.\n",
    "### case 1. the impression of the step is different from that of the last step\n",
    "### case 2. 'change of sort, filter selection, search for item, search for destination, search for poi' actions happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_diff_impressions(df):\n",
    "    selected_idx = []\n",
    "    prev_impressions = \"\"\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        step = row['step']\n",
    "        action_type = row['action_type']\n",
    "        curr_impressions = row['impressions']\n",
    "        resident_time = row['resident_time']\n",
    "\n",
    "        if step == 1:\n",
    "            first_idx = i\n",
    "        \n",
    "        if action_type in ['change of sort order', 'filter selection', 'search for item', 'search for destination', 'search for poi']:\n",
    "            first_idx = i\n",
    "        \n",
    "        if prev_impressions != curr_impressions:\n",
    "            first_idx = i\n",
    "        prev_impressions = curr_impressions\n",
    "        \n",
    "        if resident_time == -9999:\n",
    "            last_idx = i\n",
    "            selected_idx += list(range(first_idx, last_idx+1))\n",
    "            \n",
    "    return selected_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = filter_diff_impressions(new_train_df)\n",
    "filtered_train_df = pd.DataFrame(new_train_df.loc[selected_idx], columns=new_train_df.columns)\n",
    "filtered_train_df = filtered_train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = filter_diff_impressions(new_test_df)\n",
    "filtered_test_df = pd.DataFrame(new_test_df.loc[selected_idx], columns=new_test_df.columns)\n",
    "filtered_test_df = filtered_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the average price per nation or city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_lst = []\n",
    "for i in tqdm(range(len(filtered_train_df))):\n",
    "    nation_lst.append(filtered_train_df['city'][i].split(\", \")[1])\n",
    "filtered_train_df['nation'] = nation_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nation_lst = []\n",
    "for i in tqdm(range(len(filtered_test_df))):\n",
    "    test_nation_lst.append(filtered_test_df['city'][i].split(\", \")[1])\n",
    "filtered_test_df['nation'] = test_nation_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_price_dict = {}\n",
    "nations = list(set(nation_lst))\n",
    "last_step = filtered_train_df[filtered_train_df['resident_time']==-9999]\n",
    "for nation in tqdm(nations):\n",
    "    tmp = last_step[last_step['nation']==nation]\n",
    "\n",
    "    prices_all = []\n",
    "    for i in range(len(tmp)):\n",
    "        prices = [int(p) for p in tmp['prices'].iloc[i].split(\"|\")]\n",
    "        prices_all += prices\n",
    "    nation_price_dict[nation] = [np.mean(prices_all), np.median(prices_all)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_price_dict = {}\n",
    "cities = list(set(filtered_train_df['city']))\n",
    "last_step = filtered_train_df[filtered_train_df['resident_time']==-9999]\n",
    "for city in tqdm(cities):\n",
    "    tmp = last_step[last_step['city']==city]\n",
    "    \n",
    "    prices_all = []\n",
    "    for i in range(len(tmp)):\n",
    "        prices = [int(p) for p in tmp['prices'].iloc[i].split(\"|\")]\n",
    "        prices_all += prices\n",
    "    city_price_dict[city] = [np.mean(prices_all), np.median(prices_all)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_prices(df):\n",
    "    diff_city_mean_lst = []\n",
    "    diff_city_median_lst = []\n",
    "    diff_nation_mean_lst = []\n",
    "    diff_nation_median_lst = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        if row['resident_time'] == -9999:\n",
    "            city = row['city']\n",
    "            nation = row['nation']\n",
    "            prices = row['prices'].split(\"|\")\n",
    "            try:\n",
    "                city_mean = city_price_dict[city][0]\n",
    "                city_median = city_price_dict[city][1]\n",
    "                nation_mean = nation_price_dict[nation][0]\n",
    "                nation_median = nation_price_dict[nation][1]\n",
    "                diff_city_mean_lst.append('|'.join([str(int(p)-city_mean) for p in prices]))\n",
    "                diff_city_median_lst.append('|'.join([str(int(p)-city_median) for p in prices]))\n",
    "                diff_nation_mean_lst.append('|'.join([str(int(p)-nation_mean) for p in prices]))\n",
    "                diff_nation_median_lst.append('|'.join([str(int(p)-nation_median) for p in prices]))\n",
    "            except:\n",
    "                diff_city_mean_lst.append('|'.join([str(0)]*len(prices)))\n",
    "                diff_city_median_lst.append('|'.join([str(0)]*len(prices)))\n",
    "                diff_nation_mean_lst.append('|'.join([str(0)]*len(prices)))\n",
    "                diff_nation_median_lst.append('|'.join([str(0)]*len(prices)))\n",
    "        else:\n",
    "            diff_city_mean_lst.append(np.nan)\n",
    "            diff_city_median_lst.append(np.nan)\n",
    "            diff_nation_mean_lst.append(np.nan)\n",
    "            diff_nation_median_lst.append(np.nan)\n",
    "    return diff_city_mean_lst, diff_city_median_lst, diff_nation_mean_lst, diff_nation_median_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_df['diff_city_mean'], filtered_train_df['diff_city_median'], filtered_train_df['diff_nation_mean'], filtered_train_df['diff_nation_median'] = get_diff_prices(filtered_train_df)\n",
    "filtered_test_df['diff_city_mean'], filtered_test_df['diff_city_median'], filtered_test_df['diff_nation_mean'], filtered_test_df['diff_nation_median'] = get_diff_prices(filtered_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_df.to_csv('data/train_final.csv', index=False)\n",
    "filtered_test_df.to_csv('data/test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From dataframe to real using data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_final.csv')\n",
    "test_df = pd.read_csv('data/test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/item_encoding_dict.pickle', 'rb') as f:\n",
    "    item_encoding_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_criteria = Counter(train_df[train_df['action_type']=='filter selection']['reference'])\n",
    "for k, v in filter_criteria.items():\n",
    "    if v > 2000:\n",
    "        print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_filters = ['Price', 'Rating', 'Distance', 'Value', 'Hotel', 'Star', 'Hostal', 'Motel', 'Apartment', 'Breakfast', 'WiFi', 'Park']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_type_lst = ['clickout item', 'interaction item rating', 'interaction item info', 'interaction item image', 'interaction item deals',\n",
    "                   'change of sort order', 'filter selection', 'search for item', 'search for destination', 'search for poi']\n",
    "numeric_action_type = ['clickout item', 'interaction item rating', 'interaction item info', 'interaction item image', 'interaction item deals', 'search for item']\n",
    "\n",
    "action_type_one_hot = {}\n",
    "for i in range(len(action_type_lst)):\n",
    "    tmp = [0]*10\n",
    "    tmp[i] = 1\n",
    "    action_type_one_hot[action_type_lst[i]] = tmp\n",
    "print (action_type_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    EMBEDDING_SIZE = 10+25+1\n",
    "    THRESHOLD_TIME = 30.0\n",
    "    with open('data/train_final.csv', encoding=\"utf-8\") as train_f:\n",
    "        rdr = csv.reader(train_f)\n",
    "        next(rdr)\n",
    "\n",
    "        sequence_lst = []\n",
    "        train_lst = []\n",
    "        display_lst = []\n",
    "        encoding_lst = []\n",
    "        device_lst = []\n",
    "        criteria_lst = []\n",
    "        label_lst = []\n",
    "\n",
    "        for idx, line in enumerate(rdr):\n",
    "            u_id = line[0]\n",
    "            s_id = line[1]\n",
    "            timestamp = line[2]\n",
    "            step = line[3]\n",
    "            action_type = line[4]\n",
    "            reference = line[5]\n",
    "            platform = line[6]\n",
    "            city = line[7]\n",
    "            device = line[8].split(\"|\")\n",
    "            current_filters = line[9]\n",
    "            impressions = line[10].split(\"|\")\n",
    "            prices = line[11].split(\"|\")\n",
    "            viewed = line[12].split(\"|\")\n",
    "            ratings = line[13].split(\"|\")\n",
    "            hotel = line[14].split(\"|\")\n",
    "            star = line[15].split(\"|\")\n",
    "            tv = line[16].split(\"|\")\n",
    "            shower = line[17].split(\"|\")\n",
    "            window = line[18].split(\"|\")\n",
    "            car = line[19].split(\"|\")\n",
    "            wifi = line[20].split(\"|\")\n",
    "            nosmoke = line[21].split(\"|\")\n",
    "            resident_time = line[22]\n",
    "            diff_prices = line[23].split(\"|\")\n",
    "            diff_ratings = line[24].split(\"|\")\n",
    "            nation = line[25]\n",
    "            diff_city_mean = line[26].split(\"|\")\n",
    "            diff_city_median = line[27].split(\"|\")\n",
    "            diff_nation_mean = line[28].split(\"|\")\n",
    "            diff_nation_median = line[29].split(\"|\")\n",
    "            \n",
    "            action_embedding = action_type_one_hot[action_type]\n",
    "            ref_id_embedding = [0]*25\n",
    "            if reference in impressions:\n",
    "                ref_idx = impressions.index(reference)\n",
    "                ref_id_embedding[ref_idx] += 1.\n",
    "\n",
    "            time_embedding = [min([int(resident_time)+1, THRESHOLD_TIME])/THRESHOLD_TIME]\n",
    "            step_embedding = action_embedding + ref_id_embedding + time_embedding\n",
    "            \n",
    "            if action_type in numeric_action_type:\n",
    "                if reference in impressions:\n",
    "                    sequence_lst.append(step_embedding)\n",
    "            else:\n",
    "                sequence_lst.append(step_embedding)\n",
    "            \n",
    "            if resident_time == \"-9999\" and len(sequence_lst)>0:\n",
    "                sequence_lst.pop()\n",
    "                if reference in impressions:\n",
    "                    sequence_lst = sequence_lst[-50:]\n",
    "                    sequence_lst = [([0] * EMBEDDING_SIZE) for _ in range((50 - len(sequence_lst)))] + sequence_lst\n",
    "                    train_lst.append(sequence_lst)\n",
    "                    \n",
    "                    idx_lst = list(range(1,len(impressions)+1))\n",
    "                    idx_lst += [0] * (25-len(idx_lst))\n",
    "                    prices = [float(p) for p in prices]\n",
    "                    prices += [0] * (25-len(prices))\n",
    "                    viewed = [int(x) for x in viewed]\n",
    "                    viewed += [0] * (25-len(viewed))\n",
    "                    ratings = [int(x) for x in ratings]\n",
    "                    ratings += [0] * (25-len(ratings))\n",
    "                    hotel = [int(x) for x in hotel]\n",
    "                    hotel += [0] * (25-len(hotel))\n",
    "                    star = [int(x) for x in star]\n",
    "                    star += [0] * (25-len(star))\n",
    "                    tv = [int(x) for x in tv]\n",
    "                    tv += [0] * (25-len(tv))\n",
    "                    shower = [int(x) for x in shower]\n",
    "                    shower += [0] * (25-len(shower))\n",
    "                    window = [int(x) for x in window]\n",
    "                    window += [0] * (25-len(window))\n",
    "                    car = [int(x) for x in car]\n",
    "                    car += [0] * (25-len(car))\n",
    "                    wifi = [int(x) for x in wifi]\n",
    "                    wifi += [0] * (25-len(wifi))\n",
    "                    nosmoke = [int(x) for x in nosmoke]\n",
    "                    nosmoke += [0] * (25-len(nosmoke))                    \n",
    "                    \n",
    "                    diff_prices = [float(x) for x in diff_prices]\n",
    "                    diff_prices += [0] * (25-len(diff_prices))\n",
    "                    diff_ratings = [float(x) for x in diff_ratings]\n",
    "                    diff_ratings += [0] * (25-len(diff_ratings))\n",
    "                    diff_city_mean = [float(x) for x in diff_city_mean]\n",
    "                    diff_city_mean += [0] * (25-len(diff_city_mean))\n",
    "                    diff_city_median = [float(x) for x in diff_city_median]\n",
    "                    diff_city_median += [0] * (25-len(diff_city_median))\n",
    "                    diff_nation_mean = [float(x) for x in diff_nation_mean]\n",
    "                    diff_nation_mean += [0] * (25-len(diff_nation_mean))\n",
    "                    diff_nation_median = [float(x) for x in diff_nation_median]\n",
    "                    diff_nation_median += [0] * (25-len(diff_nation_median))\n",
    "                    \n",
    "                    features = [idx_lst, prices, hotel, star, tv, shower, window, car, wifi, nosmoke, ratings, viewed, diff_prices, diff_ratings, diff_city_mean, diff_city_median, diff_nation_mean, diff_nation_median]\n",
    "                    display_lst.append(features)\n",
    "                    \n",
    "                    item_encoding = []\n",
    "                    for j in range(25):\n",
    "                        try:\n",
    "                            item_encoding.append(item_encoding_dict[int(impressions[j])].tolist())\n",
    "                        except:\n",
    "                            item_encoding.append([0.]*32)\n",
    "                    item_encoding = np.transpose(np.array(item_encoding), (1,0)).tolist()\n",
    "                    encoding_lst.append(item_encoding)\n",
    "                    \n",
    "                    filtered_criteria = [0]*12\n",
    "                    try:\n",
    "                        current_filters = current_filters.split(\"|\")\n",
    "                        for i in range(len(common_filters)):\n",
    "                            for fil in current_filters:\n",
    "                                if common_filters[i] in fil:\n",
    "                                    filtered_criteria[i] = 1\n",
    "                        criteria_lst.append(filtered_criteria)\n",
    "                    except:\n",
    "                        criteria_lst.append(filtered_criteria)\n",
    "                    \n",
    "                    if device == \"desktop\":\n",
    "                        device_lst.append([1,0,0])\n",
    "                    elif device == \"mobile\":\n",
    "                        device_lst.append([0,1,0])\n",
    "                    else:\n",
    "                        device_lst.append([0,0,1])\n",
    "                    \n",
    "                    label_lst.append(impressions.index(reference))\n",
    "                sequence_lst = []\n",
    "\n",
    "        with open('data/train_final.pickle', 'wb') as f:\n",
    "            pickle.dump(train_lst, f)\n",
    "        with open('data/train_display_final.pickle', 'wb') as f:\n",
    "            pickle.dump(display_lst, f)\n",
    "        with open('data/train_encoding_final.pickle', 'wb') as f:\n",
    "            pickle.dump(encoding_lst, f)\n",
    "        with open('data/train_criteria_final.pickle', 'wb') as f:\n",
    "            pickle.dump(criteria_lst, f)\n",
    "        with open('data/train_device_final.pickle', 'wb') as f:\n",
    "            pickle.dump(device_lst, f)\n",
    "        with open('data/label_final.pickle', 'wb') as f:\n",
    "            pickle.dump(label_lst, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data():\n",
    "    EMBEDDING_SIZE = 10+25+1\n",
    "    THRESHOLD_TIME = 30.0\n",
    "    with open('data/test_final.csv', encoding=\"utf-8\") as test_f:\n",
    "        rdr = csv.reader(test_f)\n",
    "        next(rdr)\n",
    "\n",
    "        sequence_lst = []\n",
    "        test_lst = []\n",
    "        display_lst = []\n",
    "        encoding_lst = []\n",
    "        criteria_lst = []\n",
    "        device_lst = []\n",
    "        line_lst = []\n",
    "\n",
    "        for idx, line in enumerate(rdr):\n",
    "            u_id = line[0]\n",
    "            s_id = line[1]\n",
    "            timestamp = line[2]\n",
    "            step = line[3]\n",
    "            action_type = line[4]\n",
    "            reference = line[5]\n",
    "            platform = line[6]\n",
    "            city = line[7]\n",
    "            device = line[8].split(\"|\")\n",
    "            current_filters = line[9]\n",
    "            impressions = line[10].split(\"|\")\n",
    "            prices = line[11].split(\"|\")\n",
    "            viewed = line[12].split(\"|\")\n",
    "            ratings = line[13].split(\"|\")\n",
    "            hotel = line[14].split(\"|\")\n",
    "            star = line[15].split(\"|\")\n",
    "            tv = line[16].split(\"|\")\n",
    "            shower = line[17].split(\"|\")\n",
    "            window = line[18].split(\"|\")\n",
    "            car = line[19].split(\"|\")\n",
    "            wifi = line[20].split(\"|\")\n",
    "            nosmoke = line[21].split(\"|\")\n",
    "            resident_time = line[22]\n",
    "            diff_prices = line[23].split(\"|\")\n",
    "            diff_ratings = line[24].split(\"|\")\n",
    "            nation = line[25]\n",
    "            diff_city_mean = line[26].split(\"|\")\n",
    "            diff_city_median = line[27].split(\"|\")\n",
    "            diff_nation_mean = line[28].split(\"|\")\n",
    "            diff_nation_median = line[29].split(\"|\")\n",
    "            \n",
    "            action_embedding = action_type_one_hot[action_type]\n",
    "            ref_id_embedding = [0]*25\n",
    "            if reference in impressions:\n",
    "                ref_idx = impressions.index(reference)\n",
    "                ref_id_embedding[ref_idx] += 1.\n",
    "\n",
    "            time_embedding = [min([int(resident_time)+1, THRESHOLD_TIME])/THRESHOLD_TIME]\n",
    "            step_embedding = action_embedding + ref_id_embedding + time_embedding\n",
    "            \n",
    "            if action_type in numeric_action_type:\n",
    "                if reference in impressions:\n",
    "                    sequence_lst.append(step_embedding)\n",
    "            else:\n",
    "                sequence_lst.append(step_embedding)\n",
    "            \n",
    "            if resident_time == \"-9999\":\n",
    "                sequence_lst = sequence_lst[-50:]\n",
    "                sequence_lst = [([0] * EMBEDDING_SIZE) for _ in range((50 - len(sequence_lst)))] + sequence_lst\n",
    "                test_lst.append(sequence_lst)    \n",
    "            \n",
    "                idx_lst = list(range(1,len(impressions)+1))\n",
    "                idx_lst += [0] * (25-len(idx_lst))\n",
    "                prices = [float(p) for p in prices]\n",
    "                prices += [0] * (25-len(prices))\n",
    "                viewed = [int(x) for x in viewed]\n",
    "                viewed += [0] * (25-len(viewed))\n",
    "                ratings = [int(x) for x in ratings]\n",
    "                ratings += [0] * (25-len(ratings))\n",
    "                hotel = [int(x) for x in hotel]\n",
    "                hotel += [0] * (25-len(hotel))\n",
    "                star = [int(x) for x in star]\n",
    "                star += [0] * (25-len(star))\n",
    "                tv = [int(x) for x in tv]\n",
    "                tv += [0] * (25-len(tv))\n",
    "                shower = [int(x) for x in shower]\n",
    "                shower += [0] * (25-len(shower))\n",
    "                window = [int(x) for x in window]\n",
    "                window += [0] * (25-len(window))\n",
    "                car = [int(x) for x in car]\n",
    "                car += [0] * (25-len(car))\n",
    "                wifi = [int(x) for x in wifi]\n",
    "                wifi += [0] * (25-len(wifi))\n",
    "                nosmoke = [int(x) for x in nosmoke]\n",
    "                nosmoke += [0] * (25-len(nosmoke))\n",
    "                \n",
    "                diff_prices = [float(p) for p in diff_prices]\n",
    "                diff_prices += [0] * (25-len(diff_prices))\n",
    "                diff_ratings = [float(r) for r in diff_ratings]\n",
    "                diff_ratings += [0] * (25-len(diff_ratings))\n",
    "                diff_city_mean = [float(x) for x in diff_city_mean]\n",
    "                diff_city_mean += [0] * (25-len(diff_city_mean))\n",
    "                diff_city_median = [float(x) for x in diff_city_median]\n",
    "                diff_city_median += [0] * (25-len(diff_city_median))\n",
    "                diff_nation_mean = [float(x) for x in diff_nation_mean]\n",
    "                diff_nation_mean += [0] * (25-len(diff_nation_mean))\n",
    "                diff_nation_median = [float(x) for x in diff_nation_median]\n",
    "                diff_nation_median += [0] * (25-len(diff_nation_median))\n",
    "                    \n",
    "                features = [idx_lst, prices, hotel, star, tv, shower, window, car, wifi, nosmoke, ratings, viewed, diff_prices, diff_ratings, diff_city_mean, diff_city_median, diff_nation_mean, diff_nation_median]\n",
    "                display_lst.append(features)\n",
    "                \n",
    "                item_encoding = []\n",
    "                for j in range(25):\n",
    "                    try:\n",
    "                        item_encoding.append(item_encoding_dict[int(impressions[j])].tolist())\n",
    "                    except:\n",
    "                        item_encoding.append([0.]*32)\n",
    "                item_encoding = np.transpose(np.array(item_encoding), (1,0)).tolist()\n",
    "                encoding_lst.append(item_encoding)\n",
    "                \n",
    "                filtered_criteria = [0]*12\n",
    "                try:\n",
    "                    current_filters = current_filters.split(\"|\")\n",
    "                    for i in range(len(common_filters)):\n",
    "                        for fil in current_filters:\n",
    "                            if common_filters[i] in fil:\n",
    "                                filtered_criteria[i] = 1\n",
    "                    criteria_lst.append(filtered_criteria)\n",
    "                except:\n",
    "                    criteria_lst.append(filtered_criteria)\n",
    "                        \n",
    "                if device == \"desktop\":\n",
    "                    device_lst.append([1,0,0])\n",
    "                elif device == \"mobile\":\n",
    "                    device_lst.append([0,1,0])\n",
    "                else:\n",
    "                    device_lst.append([0,0,1])\n",
    "                                      \n",
    "                line_lst.append([u_id, s_id, timestamp, step, impressions])\n",
    "                sequence_lst = []\n",
    "\n",
    "        with open('data/test_final.pickle', 'wb') as f:\n",
    "            pickle.dump(test_lst, f)\n",
    "        with open('data/test_display_final.pickle', 'wb') as f:\n",
    "            pickle.dump(display_lst, f)\n",
    "        with open('data/test_encoding_final.pickle', 'wb') as f:\n",
    "            pickle.dump(encoding_lst, f)\n",
    "        with open('data/test_criteria_final.pickle', 'wb') as f:\n",
    "            pickle.dump(criteria_lst, f)\n",
    "        with open('data/test_device_final.pickle', 'wb') as f:\n",
    "            pickle.dump(device_lst, f)\n",
    "        with open('data/line_final.pickle', 'wb') as f:\n",
    "            pickle.dump(line_lst, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... create train data ...\")\n",
    "create_train_data()\n",
    "print(\"... end train data ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... create test data ...\")\n",
    "create_test_data()\n",
    "print(\"... end test data ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature nomalization and extension (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_final.pickle', 'rb') as f:\n",
    "    sessions = pickle.load(f)\n",
    "with open('data/train_display_final.pickle', 'rb') as f:\n",
    "    displays = pickle.load(f)\n",
    "with open('data/train_encoding_final.pickle', 'rb') as f:\n",
    "    encodings = pickle.load(f)\n",
    "with open('data/train_criteria_final.pickle', 'rb') as f:\n",
    "    criteria = pickle.load(f)\n",
    "with open('data/train_device_final.pickle', 'rb') as f:\n",
    "    devices = pickle.load(f)\n",
    "with open('data/label_final.pickle', 'rb') as f:\n",
    "    clicked_item = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max length as 15\n",
    "for i in range(len(sessions)):\n",
    "    sessions[i] = sessions[i][35:]\n",
    "\n",
    "# For reciprocal price,\n",
    "price = np.array(displays)[:,1,:].tolist()\n",
    "\n",
    "# display:\n",
    "# idx(0), price(1), hotel(2), star(3), tv(4), shower(5), window(6), car(7), wifi(8), nosmoke(9), ratings(10)\n",
    "# viewed(11), diff_prices(12), diff_ratings(13)\n",
    "# diff_city_mean(14), diff_city_median(15), diff_nation_mean(16), diff_nation_median(17)\n",
    "\n",
    "# Normalize\n",
    "PRICE_THRESHOLD = 1000\n",
    "for i in range(len(displays)):\n",
    "    displays[i][0] = [1/p if p!=0 else 0 for p in displays[i][0]]\n",
    "    displays[i][1] = [p/PRICE_THRESHOLD if p<PRICE_THRESHOLD else 1 for p in displays[i][1]]\n",
    "    displays[i][3] = [r/5 for r in displays[i][3]]\n",
    "    displays[i][10] = [r/5 for r in displays[i][10]]\n",
    "    \n",
    "    max_diff = max([max(displays[i][12]),-min(displays[i][12])])\n",
    "    if max_diff != 0:\n",
    "        displays[i][12] = [p/max_diff for p in displays[i][12]]\n",
    "        \n",
    "    displays[i][14] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in displays[i][14]]\n",
    "    displays[i][15] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in displays[i][15]]\n",
    "    displays[i][16] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in displays[i][16]]\n",
    "    displays[i][17] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in displays[i][17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extension\n",
    "# idx_sqrt(18), idx_square(19), price_sqrt(20), price_square(21)\n",
    "# star_sqrt(22) start_square(23), rating_sqrt(24), rating_square(25)\n",
    "# price_diff_sqrt(26), price_diff_square(27), price_mean_on_display(28), price_median_on_display(29)\n",
    "# diff_city_mean_sqrt(30), diff_city_mean_square(31), diff_city_median_sqrt(32), diff_city_median_square(33)\n",
    "# diff_nation_mean_sqrt(34), diff_nation_mean_square(35), diff_nation_median_sqrt(36), diff_nation_median_square(37)\n",
    "# reciprocal_price(38)\n",
    "for i in range(len(displays)):\n",
    "    idx_sqrt = np.sqrt(displays[i][0]).tolist()\n",
    "    idx_square = np.square(displays[i][0]).tolist()\n",
    "    price_sqrt = np.sqrt(displays[i][1]).tolist()\n",
    "    price_square = np.square(displays[i][1]).tolist()\n",
    "    star_sqrt = np.sqrt(displays[i][3]).tolist()\n",
    "    star_square = np.square(displays[i][3]).tolist()\n",
    "    rating_sqrt = np.sqrt(displays[i][10]).tolist()\n",
    "    rating_square = np.square(displays[i][10]).tolist()\n",
    "\n",
    "    price_diff_sqrt = (np.where(np.array(displays[i][12])>=0,1,-1)*np.sqrt(np.abs(displays[i][12]))).tolist()\n",
    "    price_diff_square = (np.where(np.array(displays[i][12])>=0,1,-1)*np.square(displays[i][12])).tolist()\n",
    "    price_mean_on_display = (np.array(displays[i][1])-np.mean(displays[i][1])).tolist()\n",
    "    price_median_on_display = (np.array(displays[i][1])-np.median(displays[i][1])).tolist()\n",
    "\n",
    "    diff_city_mean_sqrt = (np.where(np.array(displays[i][14])>=0,1,-1)*np.sqrt(np.abs(displays[i][14]))).tolist()\n",
    "    diff_city_mean_square = (np.where(np.array(displays[i][14])>=0,1,-1)*np.square(displays[i][14])).tolist()\n",
    "    diff_city_median_sqrt = (np.where(np.array(displays[i][15])>=0,1,-1)*np.sqrt(np.abs(displays[i][15]))).tolist()\n",
    "    diff_city_median_square = (np.where(np.array(displays[i][15])>=0,1,-1)*np.square(displays[i][15])).tolist()\n",
    "    diff_nation_mean_sqrt = (np.where(np.array(displays[i][16])>=0,1,-1)*np.sqrt(np.abs(displays[i][16]))).tolist()\n",
    "    diff_nation_mean_square = (np.where(np.array(displays[i][16])>=0,1,-1)*np.square(displays[i][16])).tolist()\n",
    "    diff_nation_median_sqrt = (np.where(np.array(displays[i][17])>=0,1,-1)*np.sqrt(np.abs(displays[i][17]))).tolist()\n",
    "    diff_nation_median_square = (np.where(np.array(displays[i][17])>=0,1,-1)*np.square(displays[i][17])).tolist()\n",
    "\n",
    "    reciprocal_price = [1/p if p!=0 else 0 for p in price[i]]\n",
    "\n",
    "    displays[i].extend([idx_sqrt, idx_square, price_sqrt, price_square, star_sqrt, star_square, rating_sqrt, rating_square])\n",
    "    displays[i].extend([price_diff_sqrt, price_diff_square, price_mean_on_display, price_median_on_display])\n",
    "    displays[i].extend([diff_city_mean_sqrt, diff_city_mean_square, diff_city_median_sqrt, diff_city_median_square, diff_nation_mean_sqrt, diff_nation_mean_square, diff_nation_median_sqrt, diff_nation_median_square, reciprocal_price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(zip(np.array(sessions), np.array(displays), np.array(encodings), np.array(criteria), np.array(devices), np.array(clicked_item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train_dataset.pickle\", \"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature nomalization and extension (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_final.pickle', 'rb') as f:\n",
    "    test_sessions = pickle.load(f)\n",
    "with open('data/test_display_final.pickle', 'rb') as f:\n",
    "    test_displays = pickle.load(f)\n",
    "with open('data/test_encoding_final.pickle', 'rb') as f:\n",
    "    test_encodings = pickle.load(f)\n",
    "with open('data/test_criteria_final.pickle', 'rb') as f:\n",
    "    test_criteria = pickle.load(f)\n",
    "with open('data/test_device_final.pickle', 'rb') as f:\n",
    "    test_devices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max length as 15\n",
    "for i in range(len(test_sessions)):\n",
    "    test_sessions[i] = test_sessions[i][35:]\n",
    "\n",
    "# For reciprocal price,\n",
    "test_price = np.array(test_displays)[:,1,:].tolist()\n",
    "\n",
    "# display:\n",
    "# idx(0), price(1), hotel(2), star(3), tv(4), shower(5), window(6), car(7), wifi(8), nosmoke(9), ratings(10)\n",
    "# viewed(11), diff_prices(12), diff_ratings(13)\n",
    "# diff_city_mean(14), diff_city_median(15), diff_nation_mean(16), diff_nation_median(17)\n",
    "\n",
    "# Normalize\n",
    "PRICE_THRESHOLD = 1000\n",
    "for i in range(len(test_displays)):\n",
    "    test_displays[i][0] = [1/p if p!=0 else 0 for p in test_displays[i][0]]\n",
    "    test_displays[i][1] = [p/PRICE_THRESHOLD if p<PRICE_THRESHOLD else 1 for p in test_displays[i][1]]\n",
    "    test_displays[i][3] = [r/5 for r in test_displays[i][3]]\n",
    "    test_displays[i][10] = [r/5 for r in test_displays[i][10]]\n",
    "    \n",
    "    max_diff = max([max(test_displays[i][12]),-min(test_displays[i][12])])\n",
    "    if max_diff != 0:\n",
    "        test_displays[i][12] = [p/max_diff for p in test_displays[i][12]]\n",
    "        \n",
    "    test_displays[i][14] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in test_displays[i][14]]\n",
    "    test_displays[i][15] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in test_displays[i][15]]\n",
    "    test_displays[i][16] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in test_displays[i][16]]\n",
    "    test_displays[i][17] = [p/PRICE_THRESHOLD if np.abs(p)<PRICE_THRESHOLD else p/np.abs(p) for p in test_displays[i][17]]\n",
    "    \n",
    "test_displays = np.nan_to_num(test_displays).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extension\n",
    "# idx_sqrt(18), idx_square(19), price_sqrt(20), price_square(21)\n",
    "# star_sqrt(22) start_square(23), rating_sqrt(24), rating_square(25)\n",
    "# price_diff_sqrt(26), price_diff_square(27), price_mean_on_display(28), price_median_on_display(29)\n",
    "# diff_city_mean_sqrt(30), diff_city_mean_square(31), diff_city_median_sqrt(32), diff_city_median_square(33)\n",
    "# diff_nation_mean_sqrt(34), diff_nation_mean_square(35), diff_nation_median_sqrt(36), diff_nation_median_square(37)\n",
    "# reciprocal_price(38)\n",
    "for i in range(len(test_displays)):\n",
    "    idx_sqrt = np.sqrt(test_displays[i][0]).tolist()\n",
    "    idx_square = np.square(test_displays[i][0]).tolist()\n",
    "    price_sqrt = np.sqrt(test_displays[i][1]).tolist()\n",
    "    price_square = np.square(test_displays[i][1]).tolist()\n",
    "    star_sqrt = np.sqrt(test_displays[i][3]).tolist()\n",
    "    star_square = np.square(test_displays[i][3]).tolist()\n",
    "    rating_sqrt = np.sqrt(test_displays[i][10]).tolist()\n",
    "    rating_square = np.square(test_displays[i][10]).tolist()\n",
    "\n",
    "    price_diff_sqrt = (np.where(np.array(test_displays[i][12])>=0,1,-1)*np.sqrt(np.abs(test_displays[i][12]))).tolist()\n",
    "    price_diff_square = (np.where(np.array(test_displays[i][12])>=0,1,-1)*np.square(test_displays[i][12])).tolist()\n",
    "    price_mean_on_display = (np.array(test_displays[i][1])-np.mean(test_displays[i][1])).tolist()\n",
    "    price_median_on_display = (np.array(test_displays[i][1])-np.median(test_displays[i][1])).tolist()\n",
    "\n",
    "    diff_city_mean_sqrt = (np.where(np.array(test_displays[i][14])>=0,1,-1)*np.sqrt(np.abs(test_displays[i][14]))).tolist()\n",
    "    diff_city_mean_square = (np.where(np.array(test_displays[i][14])>=0,1,-1)*np.square(test_displays[i][14])).tolist()\n",
    "    diff_city_median_sqrt = (np.where(np.array(test_displays[i][15])>=0,1,-1)*np.sqrt(np.abs(test_displays[i][15]))).tolist()\n",
    "    diff_city_median_square = (np.where(np.array(test_displays[i][15])>=0,1,-1)*np.square(test_displays[i][15])).tolist()\n",
    "    diff_nation_mean_sqrt = (np.where(np.array(test_displays[i][16])>=0,1,-1)*np.sqrt(np.abs(test_displays[i][16]))).tolist()\n",
    "    diff_nation_mean_square = (np.where(np.array(test_displays[i][16])>=0,1,-1)*np.square(test_displays[i][16])).tolist()\n",
    "    diff_nation_median_sqrt = (np.where(np.array(test_displays[i][17])>=0,1,-1)*np.sqrt(np.abs(test_displays[i][17]))).tolist()\n",
    "    diff_nation_median_square = (np.where(np.array(test_displays[i][17])>=0,1,-1)*np.square(test_displays[i][17])).tolist()\n",
    "\n",
    "    reciprocal_price = [1/p if p!=0 else 0 for p in test_price[i]]\n",
    "\n",
    "    test_displays[i].extend([idx_sqrt, idx_square, price_sqrt, price_square, star_sqrt, star_square, rating_sqrt, rating_square])\n",
    "    test_displays[i].extend([price_diff_sqrt, price_diff_square, price_mean_on_display, price_median_on_display])\n",
    "    test_displays[i].extend([diff_city_mean_sqrt, diff_city_mean_square, diff_city_median_sqrt, diff_city_median_square, diff_nation_mean_sqrt, diff_nation_mean_square, diff_nation_median_sqrt, diff_nation_median_square, reciprocal_price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = list(zip(np.array(test_sessions), np.array(test_displays), np.array(test_encodings), np.array(test_criteria), np.array(test_devices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_dataset.pickle\", \"wb\") as f:\n",
    "    pickle.dump(test_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
